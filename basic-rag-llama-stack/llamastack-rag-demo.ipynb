{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LlamaStack RAG Capabilities Demo\n",
    "\n",
    "This notebook demonstrates the Retrieval-Augmented Generation (RAG) capabilities of Llama Stack using the built-in Agent API with the `file_search` tool.\n",
    "\n",
    "## How RAG Works in Llama Stack\n",
    "\n",
    "Llama Stack provides native RAG support through its **Agent API** with the `file_search` tool:\n",
    "\n",
    "1. **Vector Store Creation**: Documents are uploaded and chunked into smaller pieces\n",
    "2. **Embedding Generation**: Each chunk is converted to a vector embedding using an embedding model\n",
    "3. **Vector Search**: When a query is made, it's embedded and compared against stored chunks using:\n",
    "   - **Semantic search**: Vector similarity (cosine/dot product)\n",
    "   - **Keyword search**: BM25 algorithm\n",
    "   - **Hybrid search**: Combination of both (configurable weights)\n",
    "4. **Retrieval**: Top matching chunks are retrieved based on relevance scores\n",
    "5. **Generation**: The LLM uses retrieved context to generate answers, citing sources\n",
    "\n",
    "### Architecture\n",
    "\n",
    "```\n",
    "User Query → Embedding → Vector Search → Top K Chunks → LLM + Context → Answer\n",
    "                              ↓\n",
    "                        Vector Store\n",
    "                     (Embeddings + Metadata)\n",
    "```\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before running this notebook:\n",
    "\n",
    "1. Install required Python packages:\n",
    "   ```bash\n",
    "   pip install llama-stack-client python-dotenv requests\n",
    "   ```\n",
    "\n",
    "2. Start your Llama Stack server (if not already running)\n",
    "\n",
    "3. Configure your environment variables in `.env` file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set up the connection to your Llama Stack server and configure the inference model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from llama_stack_client import LlamaStackClient, Agent, AgentEventLogger\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import time\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Get configuration from environment\n",
    "LLAMA_STACK_BASE_URL = os.getenv(\"LLAMA_STACK_BASE_URL\", \"http://localhost:8321\")\n",
    "INFERENCE_MODEL = os.getenv(\"INFERENCE_MODEL\", \"vllm/qwen3-14b-gaudi\")\n",
    "\n",
    "print(f\"Llama Stack URL: {LLAMA_STACK_BASE_URL}\")\n",
    "print(f\"Inference Model: {INFERENCE_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Llama Stack Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize client\n",
    "client = LlamaStackClient(base_url=LLAMA_STACK_BASE_URL)\n",
    "\n",
    "print(\"✓ Llama Stack client initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create Vector Store\n",
    "\n",
    "First, we'll create a vector store with hybrid search capabilities (combining semantic and keyword search). We'll download an HR benefits document and ingest it into the vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vector store with embedding model configuration and hybrid search\n",
    "vector_store_name = \"hr-benefits-hybrid\"\n",
    "\n",
    "vs = client.vector_stores.create(\n",
    "    name=vector_store_name,\n",
    "    extra_body={\n",
    "        \"embedding_model\": \"sentence-transformers/nomic-ai/nomic-embed-text-v1.5\",\n",
    "        \"embedding_dimension\": 768,\n",
    "        \"search_mode\": \"hybrid\",  # Enable hybrid search (keyword + semantic)\n",
    "        \"bm25_weight\": 0.5,  # Weight for keyword search (BM25)\n",
    "        \"semantic_weight\": 0.5,  # Weight for semantic search\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"✓ Vector store created: {vs.id}\")\n",
    "print(f\"  Name: {vs.name}\")\n",
    "print(f\"  Search Mode: hybrid (BM25 + semantic)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and Ingest Document\n",
    "\n",
    "We'll download the FantaCo HR Benefits document and upload it to the vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download clean text file\n",
    "url = \"https://raw.githubusercontent.com/burrsutter/fantaco-redhat-one-2026/refs/heads/main/basic-rag-llama-stack/source_docs/FantaCoFabulousHRBenefits_clean.txt\"\n",
    "print(f\"Downloading text file from {url}...\")\n",
    "response = requests.get(url)\n",
    "text_content = response.text\n",
    "\n",
    "print(f\"✓ Downloaded {len(text_content)} characters of text\")\n",
    "\n",
    "# Save the text to source_docs folder for inspection\n",
    "source_docs_path = os.path.join(\"source_docs\", \"FantaCoFabulousHRBenefits_clean.txt\")\n",
    "os.makedirs(os.path.dirname(source_docs_path), exist_ok=True)\n",
    "with open(source_docs_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(text_content)\n",
    "print(f\"✓ Saved text to: {source_docs_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload as text file\n",
    "text_buffer = BytesIO(text_content.encode('utf-8'))\n",
    "text_buffer.name = \"hr-benefits-clean.txt\"\n",
    "\n",
    "uploaded_file = client.files.create(\n",
    "    file=text_buffer,\n",
    "    purpose=\"assistants\"\n",
    ")\n",
    "\n",
    "print(f\"✓ File uploaded: {uploaded_file.id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attach file to vector store with custom chunking strategy\n",
    "client.vector_stores.files.create(\n",
    "    vector_store_id=vs.id,\n",
    "    file_id=uploaded_file.id,\n",
    "    chunking_strategy={\n",
    "        \"type\": \"static\",\n",
    "        \"static\": {\n",
    "            \"max_chunk_size_tokens\": 100,\n",
    "            \"chunk_overlap_tokens\": 10\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"✓ File {uploaded_file.id} added to vector store\")\n",
    "print(\"  Chunking: 100 tokens per chunk, 10 token overlap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check file status\n",
    "time.sleep(2)\n",
    "files = client.vector_stores.files.list(vector_store_id=vs.id)\n",
    "for f in files:\n",
    "    print(f\"File status: {f.status}\")\n",
    "    if f.status == \"completed\":\n",
    "        print(\"✓ File processing completed successfully\")\n",
    "    elif f.status == \"failed\":\n",
    "        print(\"✗ File processing failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. List Available Vector Stores\n",
    "\n",
    "Let's see all the vector stores we have available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all vector stores\n",
    "vector_stores = client.vector_stores.list()\n",
    "\n",
    "print(\"Available Vector Stores:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for vs_item in vector_stores:\n",
    "    print(f\"ID: {vs_item.id}\")\n",
    "    print(f\"Name: {vs_item.name}\")\n",
    "    print(f\"Created: {vs_item.created_at}\")\n",
    "    \n",
    "    # List files in this vector store\n",
    "    files = client.vector_stores.files.list(vector_store_id=vs_item.id)\n",
    "    file_count = len(list(files))\n",
    "    print(f\"Files: {file_count}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "if not list(vector_stores):\n",
    "    print(\"No vector stores found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test RAG Queries\n",
    "\n",
    "Now let's test the RAG system with various queries about HR benefits.\n",
    "\n",
    "### Helper Functions\n",
    "\n",
    "First, we'll define some helper functions to make it easier to query and display results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_vector_store(name_pattern=\"hr-benefits-hybrid\"):\n",
    "    \"\"\"Get the most recent vector store matching the name pattern.\"\"\"\n",
    "    vector_stores = list(client.vector_stores.list())\n",
    "    matching_stores = [vs for vs in vector_stores if name_pattern in vs.name]\n",
    "    if matching_stores:\n",
    "        return max(matching_stores, key=lambda vs: vs.created_at)\n",
    "    return None\n",
    "\n",
    "def query_rag_agent(query, vector_store_id, model=INFERENCE_MODEL, stream=True):\n",
    "    \"\"\"Query the RAG agent with a question.\"\"\"\n",
    "    agent = Agent(\n",
    "        client,\n",
    "        model=model,\n",
    "        instructions=\"You MUST use the file_search tool to answer ALL questions by searching the provided documents.\",\n",
    "        tools=[\n",
    "            {\n",
    "                \"type\": \"file_search\",\n",
    "                \"vector_store_ids\": [vector_store_id],\n",
    "            }\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "    session_id = agent.create_session(f\"query-{hash(query)}\")\n",
    "    response = agent.create_turn(\n",
    "        messages=[{\"role\": \"user\", \"content\": query}],\n",
    "        session_id=session_id,\n",
    "        stream=stream,\n",
    "    )\n",
    "    \n",
    "    return response\n",
    "\n",
    "def print_agent_response(response):\n",
    "    \"\"\"Print the agent's streaming response.\"\"\"\n",
    "    for log in AgentEventLogger().log(response):\n",
    "        print(log, end=\"\")\n",
    "    print()  # Add newline at the end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Vector Store for Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the vector store (use the most recent one with matching name)\n",
    "vector_store = get_latest_vector_store(\"hr-benefits-hybrid\")\n",
    "\n",
    "if not vector_store:\n",
    "    print(\"Error: Vector store 'hr-benefits-hybrid' not found. Please run the create vector store section first.\")\n",
    "else:\n",
    "    print(f\"Using vector store: {vector_store.id}\")\n",
    "    print(f\"Using model: {INFERENCE_MODEL}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 1: General Retirement Benefits\n",
    "\n",
    "Ask a general question about retirement benefits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query1 = \"What do I receive when I retire?\"\n",
    "\n",
    "print(f\"Query: {query1}\\n\")\n",
    "print(\"Agent Response:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "response1 = query_rag_agent(query1, vector_store.id)\n",
    "print_agent_response(response1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 2: Specific Question About Gold Watch\n",
    "\n",
    "Test retrieval with a more specific query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query2 = \"When do I get my gold watch?\"\n",
    "\n",
    "print(f\"Query: {query2}\\n\")\n",
    "print(\"Agent Response:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "response2 = query_rag_agent(query2, vector_store.id)\n",
    "print_agent_response(response2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 3: Multiple Unique Terms\n",
    "\n",
    "Test retrieval quality with queries containing unique terms from the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_unique = [\n",
    "    \"Tell me about the chocolate statue and personal bard\",\n",
    "    \"What do I get instead of a gold watch when I retire\",\n",
    "    \"Tell me about the 401k and astrological alignment\",\n",
    "]\n",
    "\n",
    "for query in queries_unique:\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    response = query_rag_agent(query, vector_store.id)\n",
    "    print_agent_response(response)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Debug Vector Search\n",
    "\n",
    "Let's inspect the raw vector search results to see what chunks are being retrieved.\n",
    "\n",
    "This helps us understand:\n",
    "- What content is being found for different queries\n",
    "- Relevance scores for retrieved chunks\n",
    "- Whether the hybrid search is working effectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different queries and see what gets retrieved\n",
    "debug_queries = [\n",
    "    \"gold watch retirement\",\n",
    "    \"chocolate statue\",\n",
    "    \"401k astrological alignment\",\n",
    "    \"personal bard\",\n",
    "    \"retirement benefits\",\n",
    "]\n",
    "\n",
    "for query in debug_queries:\n",
    "    print(f\"\\nQuery: '{query}'\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    try:\n",
    "        # Direct vector store search\n",
    "        results = client.vector_stores.search(\n",
    "            vector_store_id=vector_store.id,\n",
    "            query=query\n",
    "        )\n",
    "        \n",
    "        print(f\"Search results type: {type(results)}\")\n",
    "        \n",
    "        # Try to access data if available\n",
    "        if hasattr(results, 'data'):\n",
    "            if results.data:\n",
    "                print(f\"Found {len(results.data)} results\")\n",
    "                for i, result in enumerate(results.data[:3], 1):  # Show top 3\n",
    "                    print(f\"\\n  Result {i}:\")\n",
    "                    if hasattr(result, 'score'):\n",
    "                        print(f\"    Score: {result.score}\")\n",
    "                    if hasattr(result, 'content'):\n",
    "                        content = result.content[:200] if len(result.content) > 200 else result.content\n",
    "                        print(f\"    Content: {content}...\")\n",
    "                    else:\n",
    "                        print(f\"    Data: {result}\")\n",
    "            else:\n",
    "                print(\"  No results returned\")\n",
    "        else:\n",
    "            print(f\"  Response attributes: {dir(results)}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"  Error: {e}\")\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cleanup: Delete Vector Stores\n",
    "\n",
    "When you're done testing, you can clean up by deleting vector stores.\n",
    "\n",
    "**Warning**: This will permanently delete the vector store and all its associated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_vector_stores_detailed():\n",
    "    \"\"\"List all vector stores with details.\"\"\"\n",
    "    vector_stores = list(client.vector_stores.list())\n",
    "    \n",
    "    if not vector_stores:\n",
    "        print(\"No vector stores found.\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"Found {len(vector_stores)} vector store(s):\")\n",
    "    print(\"-\" * 80)\n",
    "    for i, vs in enumerate(vector_stores, 1):\n",
    "        print(f\"{i}. Name: {vs.name}\")\n",
    "        print(f\"   ID: {vs.id}\")\n",
    "        print(f\"   Created: {vs.created_at}\")\n",
    "        print()\n",
    "    \n",
    "    return vector_stores\n",
    "\n",
    "def delete_vector_store_by_name(name_pattern):\n",
    "    \"\"\"Delete vector stores matching a name pattern.\"\"\"\n",
    "    vector_stores = list(client.vector_stores.list())\n",
    "    stores_to_delete = [vs for vs in vector_stores if name_pattern in vs.name]\n",
    "    \n",
    "    if not stores_to_delete:\n",
    "        print(f\"No vector stores match pattern '{name_pattern}'\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nDeleting {len(stores_to_delete)} vector store(s)...\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for vs in stores_to_delete:\n",
    "        try:\n",
    "            client.vector_stores.delete(vector_store_id=vs.id)\n",
    "            print(f\"✓ Deleted: {vs.name} ({vs.id})\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Failed to delete {vs.name}: {e}\")\n",
    "    \n",
    "    print(\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all vector stores\n",
    "list_vector_stores_detailed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMMENT TO DELETE: Delete vector stores matching \"hr-benefits\"\n",
    "# delete_vector_store_by_name(\"hr-benefits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we demonstrated:\n",
    "\n",
    "1. **Vector Store Creation** - Created a vector store with hybrid search (semantic + keyword)\n",
    "2. **Document Ingestion** - Downloaded and uploaded an HR benefits document with custom chunking\n",
    "3. **RAG Queries** - Asked various questions and got contextual answers from the documents\n",
    "4. **Debug Tools** - Inspected raw vector search results to understand retrieval quality\n",
    "5. **Cleanup** - Learned how to delete vector stores when done\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- Llama Stack's built-in RAG support makes it easy to build question-answering systems\n",
    "- Hybrid search combines the best of keyword (BM25) and semantic search\n",
    "- The Agent API handles all the complexity of tool calling and context management\n",
    "- Proper chunking and embedding configuration are crucial for good retrieval quality\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Experiment with different chunking strategies\n",
    "- Try different embedding models\n",
    "- Adjust hybrid search weights (bm25_weight vs semantic_weight)\n",
    "- Integrate with LangGraph for more complex agentic workflows\n",
    "\n",
    "## References\n",
    "\n",
    "- [Llama Stack Documentation](https://llama-stack.readthedocs.io/)\n",
    "- [Llama Stack Client Python SDK](https://github.com/meta-llama/llama-stack-client-python)\n",
    "- [Project README](./README.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
