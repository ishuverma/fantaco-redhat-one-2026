# LLM Configuration
API_KEY=your_api_key_here
INFERENCE_MODEL=qwen3:14b-q8_0
BASE_URL=http://localhost:11434/v1

# For vLLM instead of OpenAI, use:
# BASE_URL=http://localhost:8000/v1
# INFERENCE_MODEL=meta-llama/Llama-3.1-8B-Instruct

# Langfuse Configuration
LANGFUSE_PUBLIC_KEY=pk-lf-xxx
LANGFUSE_SECRET_KEY=sk-lf-xxx
LANGFUSE_BASE_URL=https://cloud.langfuse.com

# Application Configuration
PORT=8002
